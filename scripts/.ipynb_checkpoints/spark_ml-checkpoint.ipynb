{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import count, rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = ps.SparkContext('local[4]')\n",
    "sqlContext = ps.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ACCESS_KEY = os.environ['AWS_ACCESS_KEY']\n",
    "SECRET_KEY = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n",
    "DSI_CSTON_BUCKET = \"galvanize.dsi.capstone.alex\"\n",
    "MOUNT_PROJECT = \"dsi_capstone_s3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link = 's3a://{}:{}@{}/dsi_data/reviews_Musical_Instruments_5.json.gz'.format(ACCESS_KEY, SECRET_KEY, DSI_CSTON_BUCKET)\n",
    "amazon_df = sqlContext.read.json(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n",
      "10261\n"
     ]
    }
   ],
   "source": [
    "amazon_df.printSchema()\n",
    "print amazon_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = amazon_df.select('reviewText', 'overall')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rest_count = df.groupby(\"overall\").agg(count(\"overall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes_count = dict(rest_count.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rating_1 = df.filter(df[\"overall\"] <= 1.0).orderBy(rand()).limit(classes_count[1.0])\n",
    "rating_5 = df.filter(df[\"overall\"] >= 5.0).orderBy(rand()).limit(classes_count[1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_total = rating_1.union(rating_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_total = df_total.withColumn(\"label\", (df_total['overall']-1.0)/4.0)\n",
    "df_total.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_raw = df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<(overall - 1.0)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['overall']-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append(\"/mnt1/nltk_data/\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def preprocess_raw_text(text):\n",
    "    stopwords_ = set(stopwords.words('english'))\n",
    "    stemmer_ = PorterStemmer()\n",
    "    \n",
    "    if (text == None):\n",
    "        return []\n",
    "    \n",
    "    if (len(text) < 1):\n",
    "        return []\n",
    "    \n",
    "    if (type(text) == unicode):\n",
    "        text = text.encode('utf-8')\n",
    "        \n",
    "    unpunctuated_text = text.translate(None, string.punctuation)\n",
    "\n",
    "    tokens = word_tokenize(unpunctuated_text)\n",
    "\n",
    "    lowercased_tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    filtered_tokens = [w for w in lowercased_tokens if not w in stopwords_]\n",
    "    \n",
    "    stemmed = [stemmer_.stem(w) for w in filtered_tokens]\n",
    "    stemmed = [stemmer_.stem(w) for w in lowercased_tokens]\n",
    "\n",
    "    \n",
    "    return(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_paragraph = \"7\\. Implement `NaiveBayes` specifying the columns for features (`featuresCol`), labels (`labelCol`) and prediction (`predictionCol`). Then `.fit()` to obtain a model, and apply this model on the testing test.\"\n",
    "#preprocess_raw_text(t_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer_udf = udf(lambda x: preprocess_raw_text(x), ArrayType(StringType()))\n",
    "df_tokens = df_raw.withColumn(\"tokens\", tokenizer_udf(df_raw.reviewText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"/mnt1/nltk_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(reviewText=u'A great little package for the price, but unfortunately, the zipper failed the first time I tried zipping it up. I have a standard size acoustic guitar, but be aware, the fit was snug, but not tight.', overall=1.0, label=0.0, tokens=[u'a', u'great', u'littl', u'packag', u'for', u'the', u'price', u'but', u'unfortun', u'the', u'zipper', u'fail', u'the', u'first', u'time', u'i', u'tri', u'zip', u'it', u'up', u'i', u'have', u'a', u'standard', u'size', u'acoust', u'guitar', u'but', u'be', u'awar', u'the', u'fit', u'wa', u'snug', u'but', u'not', u'tight'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features_tf\", vocabSize=5000, minDF=10.0)\n",
    "cv_model = cv.fit(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features_tf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features_tf = cv_model.transform(df_tokens)\n",
    "df_features_tf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(reviewText=u'A great little package for the price, but unfortunately, the zipper failed the first time I tried zipping it up. I have a standard size acoustic guitar, but be aware, the fit was snug, but not tight.', overall=1.0, label=0.0, tokens=[u'a', u'great', u'littl', u'packag', u'for', u'the', u'price', u'but', u'unfortun', u'the', u'zipper', u'fail', u'the', u'first', u'time', u'i', u'tri', u'zip', u'it', u'up', u'i', u'have', u'a', u'standard', u'size', u'acoust', u'guitar', u'but', u'be', u'awar', u'the', u'fit', u'wa', u'snug', u'but', u'not', u'tight'], features_tf=SparseVector(478, {0: 4.0, 1: 2.0, 3: 2.0, 4: 1.0, 9: 1.0, 15: 3.0, 16: 1.0, 18: 1.0, 20: 1.0, 21: 1.0, 31: 1.0, 50: 1.0, 53: 1.0, 56: 1.0, 70: 1.0, 95: 1.0, 135: 1.0, 136: 1.0, 144: 1.0, 158: 1.0, 238: 1.0, 288: 1.0, 300: 1.0, 408: 1.0}))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_tf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"features_tf\", outputCol=\"features\")\n",
    "idfModel = idf.fit(df_features_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_features = idfModel.transform(df_features_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+\n",
      "|          reviewText|overall|label|              tokens|         features_tf|            features|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+\n",
      "|A great little pa...|    1.0|  0.0|[a, great, littl,...|(478,[0,1,3,4,9,1...|(478,[0,1,3,4,9,1...|\n",
      "|I thought somethi...|    1.0|  0.0|[i, thought, some...|(478,[1,2,4,11,12...|(478,[1,2,4,11,12...|\n",
      "|kripes man..the o...|    1.0|  0.0|[kripe, manth, on...|(478,[0,1,2,3,4,5...|(478,[0,1,2,3,4,5...|\n",
      "|At the time I bou...|    1.0|  0.0|[at, the, time, i...|(478,[0,1,2,3,4,5...|(478,[0,1,2,3,4,5...|\n",
      "|One of the ball b...|    1.0|  0.0|[one, of, the, ba...|(478,[0,2,5,8,12,...|(478,[0,2,5,8,12,...|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splits = df_features.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, overall: double, label: double, tokens: array<string>, features_tf: vector, features: vector]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = splits[0]\n",
    "df_test = splits[1]\n",
    "df_train.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.822222222222\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy: {}\".format(str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thetaarray = model.theta.toArray().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#thetaarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = cv_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(cv_model.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtype = [('label', 'S10'), ('neg', float), ('pos', float)]\n",
    "prob_values = [(vocab[i], np.exp(thetaarray[i,0]), np.exp(thetaarray[i,1])) for i in range(vocab_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array(prob_values, dtype = dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('the', 0.007975022396569017, 0.006937359971266315),\n",
       "       ('it', 0.007756336958215204, 0.005942090073255785),\n",
       "       ('you', 0.007297737665878048, 0.007290713975877631),\n",
       "       ('of', 0.0072614551712374365, 0.006674905604580032),\n",
       "       ('to', 0.006806027764126175, 0.006558189769655631),\n",
       "       ('on', 0.006661994749412663, 0.004973544624367517),\n",
       "       ('wa', 0.00651907637427299, 0.004438657075080826),\n",
       "       ('not', 0.006494504104117895, 0.0042864391631455435),\n",
       "       ('that', 0.006481331889211326, 0.006969833094877404),\n",
       "       ('as', 0.006459947723715366, 0.00565548839521435),\n",
       "       ('have', 0.006407384585883609, 0.005363177847669031),\n",
       "       ('i', 0.00631703200479609, 0.006171931374015065),\n",
       "       ('is', 0.006225190328295895, 0.007790816834200771),\n",
       "       ('one', 0.005903452498732205, 0.005037272747794601),\n",
       "       ('in', 0.005748869482649859, 0.006315929800839576),\n",
       "       ('thi', 0.005653630837498155, 0.005977627142031766),\n",
       "       ('a', 0.005542095248807324, 0.006753880074565554),\n",
       "       ('guitar', 0.005486795733775983, 0.007535054724842075),\n",
       "       ('be', 0.005478278994584602, 0.004952407860068206),\n",
       "       ('for', 0.005414920158593655, 0.007157166798066045),\n",
       "       ('they', 0.005358350826949716, 0.005535465336306389),\n",
       "       ('with', 0.005291407528281361, 0.007105705688691989),\n",
       "       ('but', 0.00526722524601708, 0.004076777521749438),\n",
       "       ('from', 0.00517051653564959, 0.004684091999132756),\n",
       "       ('them', 0.005154098630556709, 0.004989008641594126),\n",
       "       ('pedal', 0.005148769972074815, 0.006540648186671017),\n",
       "       ('out', 0.005113480067101547, 0.0033581423870478757),\n",
       "       ('tri', 0.005042146516266703, 0.0025297418753252327),\n",
       "       ('and', 0.005014877993726495, 0.005395989780904885),\n",
       "       ('an', 0.0049202514470150765, 0.003925841132289389),\n",
       "       ('work', 0.004884413017818836, 0.003388629460170613),\n",
       "       ('strap', 0.0048514249459743215, 0.005102144407597694),\n",
       "       ('so', 0.004802074755299242, 0.0058809428419873515),\n",
       "       ('if', 0.004729322019534285, 0.003716427526975717),\n",
       "       ('back', 0.0046997750211774, 0.0015738259980660261),\n",
       "       ('stand', 0.004635312681159353, 0.006294895996046314),\n",
       "       ('just', 0.004614160638627947, 0.004438657075080826),\n",
       "       ('all', 0.004601855606589669, 0.0038342416245805127),\n",
       "       ('buy', 0.004519939364402368, 0.0030850449652974456),\n",
       "       ('get', 0.004481447185618198, 0.003951834737770784),\n",
       "       ('my', 0.004476450388132009, 0.006697735287018824),\n",
       "       ('sound', 0.004453591927099247, 0.006205895590566337),\n",
       "       ('are', 0.004405655444554115, 0.006733239363867594),\n",
       "       ('use', 0.004312819835249856, 0.00756686352764989),\n",
       "       ('like', 0.004259201431046304, 0.003991373468246712),\n",
       "       ('dont', 0.0041654693266551905, 0.0021258095500767283),\n",
       "       ('or', 0.004165196819654147, 0.0042985808741163615),\n",
       "       ('string', 0.0041157953053419715, 0.007179632756833553),\n",
       "       ('can', 0.004109537774854021, 0.0047834485656185486),\n",
       "       ('got', 0.004026232978141141, 0.0018731882534939266)], \n",
       "      dtype=[('label', 'S10'), ('neg', '<f8'), ('pos', '<f8')])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(a, order='pos')[::-1][:50]\n",
    "np.sort(a, order='neg')[::-1][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
